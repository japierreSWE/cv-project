{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual BoW - Clustering using MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sys import getsizeof\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "car_dir = \"Car_Split\"\n",
    "car_splits = os.listdir(car_dir)\n",
    "noise_dir = \"Noise_Split\"\n",
    "noise_splits = os.listdir(noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the training splits of the car dataset\n",
    "split_path = \"/\".join((car_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((car_dir, car_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((car_dir, car_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((car_dir, car_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the training splits of the noise dataset\n",
    "split_path = \"/\".join((noise_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((noise_dir, noise_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((noise_dir, noise_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((noise_dir, noise_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image descriptor has a shape (m, n, 32). \n",
    "# Where m = number of descriptors\n",
    "# n = number of descriptor vectors for each descriptor\n",
    "# 32 - number for each vector\n",
    "\n",
    "# We need to reshape such that we get (m x n, 32)\n",
    "# np.shape(car_pickle_0[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_descriptor(pickle_file):\n",
    "    print(\"Before Extraction: \\t\", np.shape(pickle_file))\n",
    "    desc_arr = []\n",
    "    for l in pickle_file:\n",
    "        if (not l is None) and len(l) != 0:\n",
    "            for desc in l:\n",
    "                desc_arr.append(desc)\n",
    "\n",
    "    print(\"After Extraction: \\t\", np.shape(desc_arr))\n",
    "\n",
    "    return(desc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (1489715, 64)\n",
      "Before Extraction: \t (1861,)\n",
      "After Extraction: \t (2636113, 64)\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (1565370, 64)\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (2217862, 64)\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (1506032, 64)\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (2211980, 64)\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (1555876, 64)\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (2409413, 64)\n"
     ]
    }
   ],
   "source": [
    "#Using a function to reshape the images based on descriptor length for clustering\n",
    "car_desc_0 = reshape_descriptor(car_pickle_0)\n",
    "noise_desc_0 = reshape_descriptor(noise_pickle_0)\n",
    "\n",
    "car_desc_1 = reshape_descriptor(car_pickle_1)\n",
    "noise_desc_1 = reshape_descriptor(noise_pickle_1)\n",
    "\n",
    "car_desc_2 = reshape_descriptor(car_pickle_2)\n",
    "noise_desc_2 = reshape_descriptor(noise_pickle_2)\n",
    "\n",
    "car_desc_3 = reshape_descriptor(car_pickle_3)\n",
    "noise_desc_3 = reshape_descriptor(noise_pickle_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1489715, 64)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(car_desc_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_no = 1500\n",
    "iters = 1000\n",
    "bs = 32\n",
    "rs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clustering model\n",
    "kmeans_batch = MiniBatchKMeans( n_clusters = cluster_no,\n",
    "                                max_iter = iters,\n",
    "                                batch_size = bs,\n",
    "                                random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MiniBatchKMeans Clustering using Partial Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  1022.6850101947784\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  53.34075379371643\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  69.83170676231384\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  63.25464916229248\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  108.87318110466003\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  148.33403491973877\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  101.79124593734741\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  147.5459587574005\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KMeans_c_1500_b_32_rs_5_.sav\n"
     ]
    }
   ],
   "source": [
    "name = \"_\".join((\"KMeans\", \"c\", str(cluster_no), \"b\", str(bs), \"rs\", str(rs), \".sav\"))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the trained model\n",
    "pickle.dump(kmeans_batch, open(name, 'wb'))"
   ]
  }
 ]
}