{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "with open(\"KMeans_c_1500_b_32_rs_5_.sav\", 'rb') as f_name:\n",
    "    kmeans_batch = pickle.load(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image, a histogram needs to be created\n",
    "# For each descriptor, a cluster number or centroid label is given.\n",
    "# Then for the entire image, a histogram counting cluster labels is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the feature descriptors\n",
    "path_dir = os.getcwd()\n",
    "train_car_dir = \"Car_Split\"\n",
    "car_splits = os.listdir(train_car_dir)\n",
    "train_noise_dir = \"Noise_Split\"\n",
    "noise_splits = os.listdir(train_noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['car_desc_split_4.pkl',\n",
       " 'car_desc_split_3.pkl',\n",
       " 'car_desc_split_2.pkl',\n",
       " 'car_desc_split_1.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 272
    }
   ],
   "source": [
    "car_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the different splits of the car train dataset\n",
    "split_path = \"/\".join((train_car_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_car_dir, car_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_car_dir, car_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_car_dir, car_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting all the different splits of the noise train dataset\n",
    "split_path = \"/\".join((train_noise_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_noise_dir, noise_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_noise_dir, noise_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"/\".join((train_noise_dir, noise_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of cluster labels for each feature in pickle_file, using the clustering model model\n",
    "def cluster_labels(pickle_file, model):\n",
    "    start_time = time.time()\n",
    "    #pickle_file = car_descriptors\n",
    "    \n",
    "    img_cluster = []\n",
    "    for img_desc in pickle_file:\n",
    "        cluster_desc = []\n",
    "        if (not img_desc is None) and len(img_desc)> 0:\n",
    "            #for desc in img_desc:\n",
    "            cluster_desc = model.predict(img_desc)\n",
    "            img_cluster.append(cluster_desc)\n",
    "    print(time.time() - start_time)\n",
    "    return(img_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "111.20344305038452\n",
      "133.01310920715332\n",
      "\n",
      "\n",
      "66.71006321907043\n",
      "92.39143300056458\n",
      "\n",
      "\n",
      "67.10342812538147\n",
      "131.9479386806488\n",
      "\n",
      "\n",
      "121.54415702819824\n",
      "185.01462602615356\n"
     ]
    }
   ],
   "source": [
    "#get cluster labels\n",
    "car_desc_0 = cluster_labels(pickle_file = car_pickle_0, model = kmeans_batch)\n",
    "noise_desc_0 = cluster_labels(noise_pickle_0, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_1 = cluster_labels(pickle_file = car_pickle_1, model = kmeans_batch)\n",
    "noise_desc_1 = cluster_labels(pickle_file = noise_pickle_1, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_2 = cluster_labels(pickle_file = car_pickle_2, model = kmeans_batch)\n",
    "noise_desc_2 = cluster_labels(pickle_file = noise_pickle_2, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_3 = cluster_labels(pickle_file = car_pickle_3, model = kmeans_batch)\n",
    "noise_desc_3 = cluster_labels(pickle_file = noise_pickle_3, model = kmeans_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "car_clusters = np.concatenate([car_desc_0, car_desc_1, car_desc_2, car_desc_3])\n",
    "noise_clusters = np.concatenate([noise_desc_0, noise_desc_1, noise_desc_2, noise_desc_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(6554,)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(6749,)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(np.shape(car_clusters))\n",
    "display(np.shape(noise_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create frequency histogram for each list of cluster labels\n",
    "def cluster_histogram(img_clusters, n_clusters = 500):\n",
    "    start_time = time.time()\n",
    "    hist_arr = []\n",
    "    for img in img_clusters:\n",
    "        hist = np.zeros(n_clusters)\n",
    "        for cluster in img:\n",
    "            hist[cluster] += 1\n",
    "        hist_arr.append(hist)\n",
    "    print(time.time() - start_time)\n",
    "    return(hist_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18.88273787498474\n",
      "17.853725910186768\n"
     ]
    }
   ],
   "source": [
    "#create histogram\n",
    "n_cluster = 1500\n",
    "#Shouldn't take more than 3 seconds for each function\n",
    "car_hist = cluster_histogram(img_clusters = car_clusters, n_clusters = n_cluster)\n",
    "noise_hist = cluster_histogram(img_clusters = noise_clusters, n_clusters = n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.6755881309509277\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardise the histogram data so that it can be used in a classifier\n",
    "car_df_v1 = pd.DataFrame(car_hist)\n",
    "sscaler = StandardScaler()\n",
    "car_df_ss = pd.DataFrame(sscaler.fit_transform(car_df_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(car_df_v1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.DataFrame(car_hist)\n",
    "mm_scalar = MinMaxScaler()\n",
    "car_df_mm = pd.DataFrame(mm_scalar.fit_transform(car_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df = pd.DataFrame(noise_hist)\n",
    "sscaler = StandardScaler()\n",
    "noise_df_ss = pd.DataFrame(sscaler.fit_transform(noise_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df = pd.DataFrame(noise_hist)\n",
    "mm_scalar = MinMaxScaler()\n",
    "noise_df_mm = pd.DataFrame(mm_scalar.fit_transform(noise_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       0         1         2         3     4         5         6         7     \\\n",
       "0  0.000000  0.000000  0.046512  0.000000   0.0  0.000000  0.015385  0.000000   \n",
       "1  0.000000  0.000000  0.116279  0.016949   0.0  0.041667  0.000000  0.000000   \n",
       "2  0.000000  0.047619  0.000000  0.000000   0.0  0.166667  0.000000  0.000000   \n",
       "3  0.016667  0.000000  0.023256  0.016949   0.0  0.041667  0.000000  0.022727   \n",
       "4  0.000000  0.047619  0.023256  0.016949   0.0  0.000000  0.000000  0.022727   \n",
       "\n",
       "      8         9     ...  1490    1491  1492  1493     1494      1495  \\\n",
       "0  0.00000  0.000000  ...  0.10  0.1875   0.0   0.0  0.02381  0.010870   \n",
       "1  0.02439  0.044444  ...  0.00  0.0000   0.0   0.0  0.02381  0.000000   \n",
       "2  0.00000  0.022222  ...  0.05  0.1250   0.0   0.0  0.02381  0.000000   \n",
       "3  0.00000  0.000000  ...  0.10  0.0000   0.0   0.0  0.00000  0.000000   \n",
       "4  0.00000  0.000000  ...  0.05  0.0625   0.0   0.0  0.02381  0.032609   \n",
       "\n",
       "       1496      1497    1498  1499  \n",
       "0  0.000000  0.000000  0.0000   0.0  \n",
       "1  0.027778  0.000000  0.0625   0.0  \n",
       "2  0.000000  0.000000  0.0000   0.0  \n",
       "3  0.000000  0.019048  0.0000   0.0  \n",
       "4  0.000000  0.019048  0.0000   0.0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1490</th>\n      <th>1491</th>\n      <th>1492</th>\n      <th>1493</th>\n      <th>1494</th>\n      <th>1495</th>\n      <th>1496</th>\n      <th>1497</th>\n      <th>1498</th>\n      <th>1499</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.046512</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.015385</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.10</td>\n      <td>0.1875</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.02381</td>\n      <td>0.010870</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.116279</td>\n      <td>0.016949</td>\n      <td>0.0</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.02439</td>\n      <td>0.044444</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.02381</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.000000</td>\n      <td>0.0625</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.047619</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.022222</td>\n      <td>...</td>\n      <td>0.05</td>\n      <td>0.1250</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.02381</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.016667</td>\n      <td>0.000000</td>\n      <td>0.023256</td>\n      <td>0.016949</td>\n      <td>0.0</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n      <td>0.022727</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.10</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.019048</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.047619</td>\n      <td>0.023256</td>\n      <td>0.016949</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.022727</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.05</td>\n      <td>0.0625</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.02381</td>\n      <td>0.032609</td>\n      <td>0.000000</td>\n      <td>0.019048</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1500 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 286
    }
   ],
   "source": [
    "car_df_mm.head()"
   ]
  },
  {
   "source": [
    "sample_car_mm = car_df_mm\n",
    "sample_noise_mm = noise_df_mm\n",
    "\n",
    "sample_car_ss = car_df_ss\n",
    "sample_noise_ss = noise_df_ss"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 288,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\n"
     ]
    }
   ],
   "source": [
    "# It happens that sometimes there are no freq found for that particular cluster\n",
    "print(np.any(sample_car_mm.isna())) \n",
    "print(np.any(sample_noise_mm.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_car = sample_car.fillna(value = -1)\n",
    "# sample_noise = sample_noise.fillna(value = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.any(sample_car_mm.isna())) \n",
    "# print(np.any(sample_noise_mm.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save histogram data to csv files\n",
    "sample_car_mm.to_csv(\"Car_Hist_MinMax_Max1500_v4.csv\", sep = ',', index= False)\n",
    "sample_car_ss.to_csv(\"Car_Hist_SS_Max1500_v4.csv\", sep = ',', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_noise_mm.to_csv(\"Noise_Hist_MinMax_Max1500_v4.csv\", sep = ',', index= False)\n",
    "sample_noise_ss.to_csv(\"Noise_Hist_SS_Max1500_v4.csv\", sep = ',', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}